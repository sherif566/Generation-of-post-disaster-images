{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEaZz6YArU7N",
        "outputId": "2552bdcb-7b59-42d7-d446-bb7c3695f90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU, UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from collections import Counter\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(image_path, image_size=(256, 256)):\n",
        "    pre_images = []\n",
        "    post_images = []\n",
        "    pre_paths = []\n",
        "    post_paths = []\n",
        "    skiping = []\n",
        "    for filename in os.listdir(image_path):\n",
        "        if 'pre_disaster' in filename:\n",
        "            pre_full_path = os.path.join(image_path, filename)\n",
        "            post_full_path = pre_full_path.replace('pre_disaster', 'post_disaster')\n",
        "\n",
        "            # Check if both pre and post disaster files exist before opening\n",
        "            if os.path.exists(pre_full_path) and os.path.exists(post_full_path):\n",
        "                try:\n",
        "                    image = Image.open(pre_full_path)\n",
        "                    post_image = Image.open(post_full_path)\n",
        "\n",
        "                    # Append the images and their paths only if both files are successfully opened\n",
        "                    pre_images.append(np.array(image.resize(image_size)))\n",
        "                    pre_paths.append(pre_full_path)\n",
        "                    post_images.append(np.array(post_image.resize(image_size)))\n",
        "                    post_paths.append(post_full_path)\n",
        "\n",
        "                except IOError as e:\n",
        "                    print(f\"Error opening one of the images {pre_full_path} or {post_full_path}: {e}\")\n",
        "            else:\n",
        "                # skiping.append(filename)\n",
        "                # print(f\"Skipping {filename} as the corresponding post disaster image does not exist.\")\n",
        "                os.remove(pre_full_path)  # This line removes the pre-disaster image file\n",
        "                print(f\"Removed {pre_full_path} due to missing corresponding post disaster image.\")\n",
        "\n",
        "    return np.array(pre_images), np.array(post_images), pre_paths, post_paths\n",
        "\n",
        "image_path = '/content/drive/My Drive/XBD_train_Data/train/images'\n",
        "pre_images, post_images, pre_paths, post_paths = load_images(image_path)\n",
        "# print(skiping)"
      ],
      "metadata": {
        "id": "wYVbUFD3yaAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gen = len(post_images)\n"
      ],
      "metadata": {
        "id": "X6KGzr9wEuC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_images.shape"
      ],
      "metadata": {
        "id": "vv1xmQOK0Q2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0748170f-555e-4122-b50b-33ad49d98622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1627, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128 * 64 * 64, activation=\"relu\", input_dim=100, kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(Reshape((64, 64, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=4, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=4, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(3, kernel_size=4, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4HjOctpDtzeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(256, 256, 3), padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XGBlddHHuhza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "    discriminator.trainable = False\n",
        "    z = Input(shape=(100,))\n",
        "    img = generator(z)\n",
        "    validity = discriminator(img)\n",
        "    combined = Model(z, validity)\n",
        "    combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "    return combined\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = build_gan(generator, discriminator)\n"
      ],
      "metadata": {
        "id": "I_1tYjU_uhv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(epoch, generator, image_save_dir='/content/drive/My Drive/GAN_Images'):\n",
        "    noise = np.random.normal(0, 1, (1, 100))  # Generate one image\n",
        "    gen_img = generator.predict(noise)\n",
        "    gen_img = 0.5 * gen_img + 0.5  # Rescale image from [-1, 1] to [0, 1]\n",
        "\n",
        "    if not os.path.exists(image_save_dir):\n",
        "        os.makedirs(image_save_dir)\n",
        "\n",
        "    img_path = f\"{image_save_dir}/epoch_{epoch}_image.png\"  # Single image per epoch\n",
        "    Image.fromarray((gen_img[0] * 255).astype(np.uint8)).save(img_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "QMgjvYO-uho_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(generator, discriminator, combined, epochs, batch_size=128, save_interval=50):\n",
        "    # Load and preprocess the dataset (placeholder function load_images needs to be defined or replaced)\n",
        "    pre_images, post_images, pre_paths, post_paths = load_images(image_path)\n",
        "    pre_images = (pre_images.astype(np.float32) - 127.5) / 127.5\n",
        "    post_images = (post_images.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, pre_images.shape[0], batch_size)\n",
        "        imgs_pre = pre_images[idx]\n",
        "        imgs_post = post_images[idx]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(imgs_post, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "        print(f\"Epoch: {epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
        "\n",
        "        if epoch % save_interval == 0 or epoch == epochs - 1:  # Save the last image of each epoch\n",
        "            save_images(epoch, generator)\n",
        "\n"
      ],
      "metadata": {
        "id": "d758LAgnTV8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the GAN\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = build_gan(generator, discriminator)\n",
        "train(generator, discriminator, gan, epochs=600, batch_size=32, save_interval=100)"
      ],
      "metadata": {
        "id": "PrcqTgkz7NPz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}